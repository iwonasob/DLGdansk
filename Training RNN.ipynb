{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "import numpy as np\n",
    "import librosa, librosa.display, IPython.display as ipd\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "np.random.seed(1515)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "sess = tf.Session(config=config) \n",
    "    \n",
    "import keras \n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras\n",
    "\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, LSTM, GRU, Bidirectional\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET PATHS\n",
    "dataspace = '/vol/vssp/datasets/audio01/UrbanSound8K/audio/'\n",
    "projectspace = '/vol/vssp/AcousticEventsDetection/DLGdansk/UrbanSound/'\n",
    "\n",
    "metadatafile = '/vol/vssp/datasets/audio01/UrbanSound8K/metadata/UrbanSound8K.csv'\n",
    "metadata = pd.read_csv(metadatafile)\n",
    "\n",
    "hdf5_path = os.path.join(projectspace,'dataset.hdf5') \n",
    "modelfolder = os.path.join(projectspace,'models')\n",
    "scalerpath = os.path.join(projectspace,'scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "hf = h5py.File(hdf5_path, 'r')\n",
    "X_train = np.array(hf.get('X_train') )\n",
    "y_train = np.array(hf.get('y_train'))\n",
    "X_val =  np.array(hf.get('X_val'))\n",
    "y_val= np.array(hf.get('y_val'))\n",
    "X_test = np.array(hf.get('X_test'))\n",
    "y_test = np.array(hf.get('y_test'))\n",
    "\n",
    "n_features= hf.get('n_features').value\n",
    "n_frames=hf.get('max_length_samp').value\n",
    "label_list= hf.get('label_list').value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALE THE DATA\n",
    "scaler=pickle.load(open(scalerpath, 'rb'))\n",
    "\n",
    "X_train_scaled = [scaler.transform(x.T) for x in X_train]\n",
    "X_val_scaled = [scaler.transform(x.T) for x in X_val]\n",
    "X_test_scaled = [scaler.transform(x.T) for x in X_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "816"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn(n_frames=n_frames, n_features=n_features,\n",
    "                     n_classes=10):\n",
    "\n",
    "    # INPUT\n",
    "    x = Input(shape=(n_frames, n_features), dtype='float32')\n",
    "  \n",
    "    y= LSTM(120, return_sequences = True)(x)\n",
    "    #y= Dropout(0.5)(y)  \n",
    "    \n",
    "    y= LSTM(120)(y)\n",
    "    #y= Dropout(0.5)(y) \n",
    "\n",
    "    y = Dense(n_classes, activation='sigmoid')(y)\n",
    "\n",
    "    m = Model(inputs=x, outputs=y)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 173, 40)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 173, 120)          77280     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 120)               115680    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1210      \n",
      "=================================================================\n",
      "Total params: 194,170\n",
      "Trainable params: 194,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m = build_rnn(n_features=40,)\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7079 samples, validate on 816 samples\n",
      "Epoch 1/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 1.8441 - acc: 0.3009 - val_loss: 1.8088 - val_acc: 0.3064\n",
      "Epoch 2/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 1.5702 - acc: 0.4037 - val_loss: 1.6956 - val_acc: 0.3456\n",
      "Epoch 3/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 1.5223 - acc: 0.4389 - val_loss: 1.5602 - val_acc: 0.4093\n",
      "Epoch 4/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 1.3949 - acc: 0.4940 - val_loss: 1.4376 - val_acc: 0.4473\n",
      "Epoch 5/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 1.3831 - acc: 0.5005 - val_loss: 1.6010 - val_acc: 0.4142\n",
      "Epoch 6/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 1.3234 - acc: 0.5227 - val_loss: 1.4830 - val_acc: 0.4890\n",
      "Epoch 7/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 1.2526 - acc: 0.5550 - val_loss: 1.5531 - val_acc: 0.4988\n",
      "Epoch 8/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 1.2032 - acc: 0.5745 - val_loss: 1.3733 - val_acc: 0.5662\n",
      "Epoch 9/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 1.1973 - acc: 0.5737 - val_loss: 1.4658 - val_acc: 0.5086\n",
      "Epoch 10/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 1.1723 - acc: 0.5926 - val_loss: 1.6517 - val_acc: 0.4498\n",
      "Epoch 11/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 1.1294 - acc: 0.6047 - val_loss: 1.4130 - val_acc: 0.5135\n",
      "Epoch 12/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 1.0724 - acc: 0.6309 - val_loss: 1.3480 - val_acc: 0.5515\n",
      "Epoch 13/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 1.2550 - acc: 0.5769 - val_loss: 1.9962 - val_acc: 0.3297\n",
      "Epoch 14/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 1.4146 - acc: 0.4851 - val_loss: 1.6054 - val_acc: 0.4167\n",
      "Epoch 15/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 1.2279 - acc: 0.5754 - val_loss: 1.5935 - val_acc: 0.4669\n",
      "Epoch 16/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 1.2216 - acc: 0.5861 - val_loss: 1.5137 - val_acc: 0.5270\n",
      "Epoch 17/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 1.1970 - acc: 0.5874 - val_loss: 1.4133 - val_acc: 0.5380\n",
      "Epoch 18/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 1.0866 - acc: 0.6333 - val_loss: 1.5567 - val_acc: 0.5061\n",
      "Epoch 19/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 1.0570 - acc: 0.6429 - val_loss: 1.5293 - val_acc: 0.4681\n",
      "Epoch 20/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 1.0083 - acc: 0.6610 - val_loss: 1.4958 - val_acc: 0.5466\n",
      "Epoch 21/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 1.0238 - acc: 0.6607 - val_loss: 1.5016 - val_acc: 0.5221\n",
      "Epoch 22/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 1.0467 - acc: 0.6408 - val_loss: 1.6686 - val_acc: 0.4914\n",
      "Epoch 23/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.9950 - acc: 0.6591 - val_loss: 1.5355 - val_acc: 0.5061\n",
      "Epoch 24/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.9508 - acc: 0.6882 - val_loss: 1.4936 - val_acc: 0.5674\n",
      "Epoch 25/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.8928 - acc: 0.7107 - val_loss: 1.3635 - val_acc: 0.5600\n",
      "Epoch 26/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.8580 - acc: 0.7228 - val_loss: 1.4686 - val_acc: 0.5613\n",
      "Epoch 27/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.7820 - acc: 0.7483 - val_loss: 1.4888 - val_acc: 0.5588\n",
      "Epoch 28/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.7777 - acc: 0.7439 - val_loss: 1.4812 - val_acc: 0.5515\n",
      "Epoch 29/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.7562 - acc: 0.7565 - val_loss: 1.5409 - val_acc: 0.5453\n",
      "Epoch 30/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.8079 - acc: 0.7432 - val_loss: 1.7930 - val_acc: 0.4547\n",
      "Epoch 31/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.9097 - acc: 0.6930 - val_loss: 1.3858 - val_acc: 0.5686\n",
      "Epoch 32/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.7654 - acc: 0.7439 - val_loss: 1.6087 - val_acc: 0.5478\n",
      "Epoch 33/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.7521 - acc: 0.7517 - val_loss: 1.4815 - val_acc: 0.5748\n",
      "Epoch 34/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 0.6883 - acc: 0.7765 - val_loss: 1.5485 - val_acc: 0.5637\n",
      "Epoch 35/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.6586 - acc: 0.7854 - val_loss: 1.5304 - val_acc: 0.5490\n",
      "Epoch 36/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.6429 - acc: 0.7858 - val_loss: 1.5212 - val_acc: 0.5539\n",
      "Epoch 37/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.5922 - acc: 0.8099 - val_loss: 1.3937 - val_acc: 0.6091\n",
      "Epoch 38/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.6037 - acc: 0.8018 - val_loss: 1.5550 - val_acc: 0.5833\n",
      "Epoch 39/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 0.5209 - acc: 0.8302 - val_loss: 1.5909 - val_acc: 0.5735\n",
      "Epoch 40/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.5410 - acc: 0.8183 - val_loss: 1.6560 - val_acc: 0.5564\n",
      "Epoch 41/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.4885 - acc: 0.8450 - val_loss: 1.7159 - val_acc: 0.5650\n",
      "Epoch 42/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 0.5092 - acc: 0.8387 - val_loss: 1.6260 - val_acc: 0.5650\n",
      "Epoch 43/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.5303 - acc: 0.8316 - val_loss: 1.5790 - val_acc: 0.5662\n",
      "Epoch 44/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.5377 - acc: 0.8257 - val_loss: 1.5663 - val_acc: 0.5895\n",
      "Epoch 45/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.4956 - acc: 0.8450 - val_loss: 1.5613 - val_acc: 0.5956\n",
      "Epoch 46/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 0.4483 - acc: 0.8556 - val_loss: 1.4994 - val_acc: 0.6078\n",
      "Epoch 47/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 0.5182 - acc: 0.8327 - val_loss: 1.4575 - val_acc: 0.6091\n",
      "Epoch 48/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.5360 - acc: 0.8246 - val_loss: 1.6439 - val_acc: 0.5882\n",
      "Epoch 49/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.4519 - acc: 0.8531 - val_loss: 1.5779 - val_acc: 0.5931\n",
      "Epoch 50/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.4264 - acc: 0.8669 - val_loss: 1.6850 - val_acc: 0.5956\n",
      "Epoch 51/200\n",
      "7079/7079 [==============================] - 26s 4ms/step - loss: 0.4842 - acc: 0.8438 - val_loss: 1.7671 - val_acc: 0.5637\n",
      "Epoch 52/200\n",
      "7079/7079 [==============================] - 26s 4ms/step - loss: 0.4295 - acc: 0.8645 - val_loss: 1.5810 - val_acc: 0.6140\n",
      "Epoch 53/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.4941 - acc: 0.8395 - val_loss: 1.6473 - val_acc: 0.6066\n",
      "Epoch 54/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.5894 - acc: 0.8141 - val_loss: 1.5543 - val_acc: 0.5882\n",
      "Epoch 55/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 0.5792 - acc: 0.8134 - val_loss: 1.5272 - val_acc: 0.5993\n",
      "Epoch 56/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 0.4828 - acc: 0.8439 - val_loss: 1.7475 - val_acc: 0.5760\n",
      "Epoch 57/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.4495 - acc: 0.8572 - val_loss: 1.5836 - val_acc: 0.6005\n",
      "Epoch 58/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 0.3985 - acc: 0.8707 - val_loss: 1.5576 - val_acc: 0.6127\n",
      "Epoch 59/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 0.3388 - acc: 0.8909 - val_loss: 1.5665 - val_acc: 0.6324\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.3217 - acc: 0.9022 - val_loss: 1.6038 - val_acc: 0.6176\n",
      "Epoch 61/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.3410 - acc: 0.8877 - val_loss: 1.5186 - val_acc: 0.5980\n",
      "Epoch 62/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.3138 - acc: 0.8986 - val_loss: 1.5821 - val_acc: 0.6042\n",
      "Epoch 63/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.3719 - acc: 0.8760 - val_loss: 1.6257 - val_acc: 0.5735\n",
      "Epoch 64/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.4822 - acc: 0.8424 - val_loss: 1.5240 - val_acc: 0.6054\n",
      "Epoch 65/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.3604 - acc: 0.8839 - val_loss: 1.6531 - val_acc: 0.6017\n",
      "Epoch 66/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.4416 - acc: 0.8604 - val_loss: 1.6042 - val_acc: 0.5821\n",
      "Epoch 67/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.4038 - acc: 0.8633 - val_loss: 1.7257 - val_acc: 0.6054\n",
      "Epoch 68/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 0.3224 - acc: 0.8981 - val_loss: 1.7315 - val_acc: 0.6176\n",
      "Epoch 69/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.2955 - acc: 0.9093 - val_loss: 1.7960 - val_acc: 0.5907\n",
      "Epoch 70/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.2755 - acc: 0.9138 - val_loss: 1.8297 - val_acc: 0.5870\n",
      "Epoch 71/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 0.4189 - acc: 0.8642 - val_loss: 1.8430 - val_acc: 0.5637\n",
      "Epoch 72/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.3385 - acc: 0.8933 - val_loss: 1.8208 - val_acc: 0.6054\n",
      "Epoch 73/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.2805 - acc: 0.9110 - val_loss: 1.8812 - val_acc: 0.5833\n",
      "Epoch 74/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 0.3244 - acc: 0.8941 - val_loss: 1.8376 - val_acc: 0.6201\n",
      "Epoch 75/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 0.3426 - acc: 0.8933 - val_loss: 1.7943 - val_acc: 0.6054\n",
      "Epoch 76/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.2631 - acc: 0.9172 - val_loss: 1.9227 - val_acc: 0.6250\n",
      "Epoch 77/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.2418 - acc: 0.9239 - val_loss: 2.0815 - val_acc: 0.6054\n",
      "Epoch 78/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.2842 - acc: 0.9087 - val_loss: 2.0312 - val_acc: 0.5968\n",
      "Epoch 79/200\n",
      "7079/7079 [==============================] - 26s 4ms/step - loss: 0.3237 - acc: 0.8976 - val_loss: 1.9200 - val_acc: 0.6078\n",
      "Epoch 80/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.3024 - acc: 0.9022 - val_loss: 1.9705 - val_acc: 0.6054\n",
      "Epoch 81/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.2902 - acc: 0.9070 - val_loss: 1.9307 - val_acc: 0.6275\n",
      "Epoch 82/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.2432 - acc: 0.9210 - val_loss: 1.9500 - val_acc: 0.6176\n",
      "Epoch 83/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 0.2102 - acc: 0.9340 - val_loss: 1.9359 - val_acc: 0.6324\n",
      "Epoch 84/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 0.1769 - acc: 0.9434 - val_loss: 2.0350 - val_acc: 0.6299\n",
      "Epoch 85/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 0.1637 - acc: 0.9489 - val_loss: 2.0776 - val_acc: 0.6152\n",
      "Epoch 86/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.3722 - acc: 0.8798 - val_loss: 1.9049 - val_acc: 0.5956\n",
      "Epoch 87/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.2471 - acc: 0.9196 - val_loss: 1.8963 - val_acc: 0.6299\n",
      "Epoch 88/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 0.3267 - acc: 0.8973 - val_loss: 1.8528 - val_acc: 0.6189\n",
      "Epoch 89/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.2901 - acc: 0.9085 - val_loss: 1.6597 - val_acc: 0.6115\n",
      "Epoch 90/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.2050 - acc: 0.9369 - val_loss: 1.7342 - val_acc: 0.6287\n",
      "Epoch 91/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.1717 - acc: 0.9503 - val_loss: 1.9511 - val_acc: 0.5931\n",
      "Epoch 92/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.1686 - acc: 0.9480 - val_loss: 2.0136 - val_acc: 0.5980\n",
      "Epoch 93/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.1690 - acc: 0.9490 - val_loss: 1.9621 - val_acc: 0.6066\n",
      "Epoch 94/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.1574 - acc: 0.9515 - val_loss: 1.9475 - val_acc: 0.6348\n",
      "Epoch 95/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.1485 - acc: 0.9542 - val_loss: 1.9244 - val_acc: 0.6409\n",
      "Epoch 96/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 0.1498 - acc: 0.9572 - val_loss: 2.0395 - val_acc: 0.6250\n",
      "Epoch 97/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.1212 - acc: 0.9660 - val_loss: 2.1445 - val_acc: 0.6201\n",
      "Epoch 98/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.1378 - acc: 0.9614 - val_loss: 2.2513 - val_acc: 0.5907\n",
      "Epoch 99/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.1030 - acc: 0.9706 - val_loss: 2.2006 - val_acc: 0.5993\n",
      "Epoch 100/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.1370 - acc: 0.9590 - val_loss: 2.2688 - val_acc: 0.5907\n",
      "Epoch 101/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.0962 - acc: 0.9726 - val_loss: 2.1096 - val_acc: 0.5944\n",
      "Epoch 102/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.0852 - acc: 0.9774 - val_loss: 2.1761 - val_acc: 0.6017\n",
      "Epoch 103/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.0680 - acc: 0.9809 - val_loss: 2.2724 - val_acc: 0.5993\n",
      "Epoch 104/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.0659 - acc: 0.9816 - val_loss: 2.2499 - val_acc: 0.6238\n",
      "Epoch 105/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.0795 - acc: 0.9757 - val_loss: 2.0958 - val_acc: 0.6262\n",
      "Epoch 106/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.1001 - acc: 0.9713 - val_loss: 2.2741 - val_acc: 0.6152\n",
      "Epoch 107/200\n",
      "7079/7079 [==============================] - 26s 4ms/step - loss: 0.0888 - acc: 0.9719 - val_loss: 2.2083 - val_acc: 0.6091\n",
      "Epoch 108/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.0774 - acc: 0.9768 - val_loss: 2.2867 - val_acc: 0.6017\n",
      "Epoch 109/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.0757 - acc: 0.9795 - val_loss: 2.2892 - val_acc: 0.6115\n",
      "Epoch 110/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.0673 - acc: 0.9814 - val_loss: 2.3942 - val_acc: 0.6176\n",
      "Epoch 111/200\n",
      "7079/7079 [==============================] - 27s 4ms/step - loss: 0.0563 - acc: 0.9843 - val_loss: 2.3253 - val_acc: 0.6115\n",
      "Epoch 112/200\n",
      "7079/7079 [==============================] - 28s 4ms/step - loss: 0.0893 - acc: 0.9729 - val_loss: 2.3032 - val_acc: 0.5968\n",
      "Epoch 113/200\n",
      "5200/7079 [=====================>........] - ETA: 7s - loss: 0.1247 - acc: 0.9600"
     ]
    }
   ],
   "source": [
    "model_checkpoint = ModelCheckpoint(\n",
    "    os.path.join(modelfolder, 'rnn_epoch_{epoch:03d}_val_loss_{val_loss:.4f}.hdf5'),\n",
    "    monitor='val_loss', save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, verbose=1)\n",
    "callbacks = [early_stopping, model_checkpoint]\n",
    "\n",
    "history = m.fit(x=np.array(X_train_scaled), y=y_train, batch_size=130,\n",
    "                    epochs=200, verbose=True,\n",
    "                    validation_data=(np.array(X_val_scaled), y_val), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = m.predict(X_test, batch_size=130, verbose=1)\n",
    "y_predict=np.array([ np.argmax(p) for p in prediction])\n",
    "y_test_label = np.array([np.argmax(y) for y in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "acc = np.sum(y_test_label==y_predict) / float(len(y_test_label))\n",
    "print('Accuracy: {:.2f}'.format(acc))\n",
    "\n",
    "cm = confusion_matrix(y_test_label, y_predict )\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 14}, fmt='g', linewidths=.5, yticklabels=label_list,xticklabels=label_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
