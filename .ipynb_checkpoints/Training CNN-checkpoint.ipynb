{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "import numpy as np\n",
    "import librosa, librosa.display, IPython.display as ipd\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from sklearn import preprocessing\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "sess = tf.Session(config=config) \n",
    "    \n",
    "import keras \n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras\n",
    "\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, LSTM\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET PATHS\n",
    "dataspace = '/vol/vssp/datasets/audio01/UrbanSound8K/audio/'\n",
    "projectspace = '/vol/vssp/AcousticEventsDetection/DLGdansk/UrbanSound/'\n",
    "\n",
    "metadatafile = '/vol/vssp/datasets/audio01/UrbanSound8K/metadata/UrbanSound8K.csv'\n",
    "metadata = pd.read_csv(metadatafile)\n",
    "\n",
    "hdf5_path = os.path.join(projectspace,'dataset_param.hdf5') \n",
    "modelfolder = os.path.join(projectspace,'models')\n",
    "scalerpath = os.path.join(projectspace,'scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string or a number, not 'Dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-b841406aec4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdf5_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'n_features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mn_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'max_length_samp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string or a number, not 'Dataset'"
     ]
    }
   ],
   "source": [
    "hf = h5py.File(hdf5_path, 'r')\n",
    "n_features= int(hf.get('n_features'))\n",
    "n_frames=int(hf.get('max_length_samp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "hf = h5py.File(hdf5_path, 'r')\n",
    "X_train = np.array(hf.get('X_train') )\n",
    "y_train = np.array(hf.get('y_train'))\n",
    "X_val =  np.array(hf.get('X_val'))\n",
    "y_val= np.array(hf.get('y_val'))\n",
    "\n",
    "n_features= hf.get('n_features')\n",
    "n_frames=hf.get('max_length_samp')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALE THE DATA\n",
    "scaler=pickle.load(open(scalerpath, 'rb'))\n",
    "\n",
    "X_train_scaled = [scaler.transform(x.T).T for x in X_train]\n",
    "X_val_scaled = [scaler.transform(x.T).T for x in X_val]\n",
    "\n",
    "\n",
    "# CHANGE DIMENSION TO FIT KERAS\n",
    "X_train = np.expand_dims(X_train_scaled, 3)\n",
    "X_val = np.expand_dims(X_val_scaled, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 40\n",
    "n_frames=173"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(n_features=n_features, n_frames=n_frames, n_filters_cnn=48,\n",
    "                     filter_size_cnn=(3, 3), pool_size_cnn=(2,2),\n",
    "                     n_classes=10):\n",
    "\n",
    "    # INPUT\n",
    "    x = Input(shape=(n_features, n_frames, 1), dtype='float32')\n",
    "\n",
    "    # CONV 1\n",
    "    y = Conv2D(n_filters_cnn, filter_size_cnn, padding='valid',\n",
    "               activation='relu')(x)\n",
    "    y = MaxPooling2D(pool_size=pool_size_cnn, strides=None, padding='valid')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "\n",
    "    # CONV 2\n",
    "    y = Conv2D(n_filters_cnn, filter_size_cnn, padding='valid',\n",
    "               activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=pool_size_cnn, strides=None, padding='valid')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "\n",
    "    # CONV 3\n",
    "    y = Conv2D(n_filters_cnn, filter_size_cnn, padding='valid',\n",
    "               activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=pool_size_cnn, strides=None, padding='valid')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "\n",
    "    # Flatten for dense layers\n",
    "    y = Flatten()(y)\n",
    "    #y = Dropout(0.5)(y)\n",
    "    #y = Dense(n_dense_cnn, activation='relu')(y)\n",
    "\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Dense(n_classes, activation='sigmoid')(y)\n",
    "\n",
    "    m = Model(inputs=x, outputs=y)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        (None, 40, 173, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_54 (Conv2D)           (None, 38, 171, 48)       480       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_48 (MaxPooling (None, 19, 85, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 19, 85, 48)        192       \n",
      "_________________________________________________________________\n",
      "conv2d_55 (Conv2D)           (None, 17, 83, 48)        20784     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_49 (MaxPooling (None, 8, 41, 48)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_52 (Batc (None, 8, 41, 48)         192       \n",
      "_________________________________________________________________\n",
      "conv2d_56 (Conv2D)           (None, 6, 39, 48)         20784     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_50 (MaxPooling (None, 3, 19, 48)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_53 (Batc (None, 3, 19, 48)         192       \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 2736)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 2736)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                27370     \n",
      "=================================================================\n",
      "Total params: 69,994\n",
      "Trainable params: 69,706\n",
      "Non-trainable params: 288\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m = build_cnn(n_features=40,)\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5435 samples, validate on 1644 samples\n",
      "Epoch 1/200\n",
      "5435/5435 [==============================] - 3s 526us/step - loss: 0.0096 - acc: 0.9963 - val_loss: 2.6164 - val_acc: 0.7007\n",
      "Epoch 2/200\n",
      "5435/5435 [==============================] - 3s 461us/step - loss: 0.0175 - acc: 0.9948 - val_loss: 3.0392 - val_acc: 0.6800\n",
      "Epoch 3/200\n",
      "5435/5435 [==============================] - 3s 487us/step - loss: 0.0120 - acc: 0.9954 - val_loss: 2.9093 - val_acc: 0.6788\n",
      "Epoch 4/200\n",
      "5435/5435 [==============================] - 3s 561us/step - loss: 0.0051 - acc: 0.9978 - val_loss: 2.8010 - val_acc: 0.6776\n",
      "Epoch 5/200\n",
      "5435/5435 [==============================] - 3s 467us/step - loss: 0.0097 - acc: 0.9965 - val_loss: 2.7939 - val_acc: 0.6746\n",
      "Epoch 6/200\n",
      "5435/5435 [==============================] - 3s 491us/step - loss: 0.0062 - acc: 0.9982 - val_loss: 2.7070 - val_acc: 0.7001\n",
      "Epoch 7/200\n",
      "5435/5435 [==============================] - 3s 545us/step - loss: 0.0060 - acc: 0.9976 - val_loss: 2.6612 - val_acc: 0.7068\n",
      "Epoch 8/200\n",
      "5435/5435 [==============================] - 3s 508us/step - loss: 0.0055 - acc: 0.9978 - val_loss: 2.6822 - val_acc: 0.7044\n",
      "Epoch 9/200\n",
      "5435/5435 [==============================] - 3s 477us/step - loss: 0.0078 - acc: 0.9976 - val_loss: 2.7918 - val_acc: 0.6904\n",
      "Epoch 10/200\n",
      "5435/5435 [==============================] - 3s 471us/step - loss: 0.0086 - acc: 0.9965 - val_loss: 2.8331 - val_acc: 0.6873\n",
      "Epoch 11/200\n",
      "5435/5435 [==============================] - 3s 591us/step - loss: 0.0083 - acc: 0.9969 - val_loss: 2.7882 - val_acc: 0.7032\n",
      "Epoch 12/200\n",
      "5435/5435 [==============================] - 3s 475us/step - loss: 0.0090 - acc: 0.9971 - val_loss: 2.6360 - val_acc: 0.7007\n",
      "Epoch 13/200\n",
      "5435/5435 [==============================] - 3s 480us/step - loss: 0.0053 - acc: 0.9982 - val_loss: 2.4841 - val_acc: 0.7013\n",
      "Epoch 14/200\n",
      "5435/5435 [==============================] - 3s 522us/step - loss: 0.0064 - acc: 0.9974 - val_loss: 2.6562 - val_acc: 0.7026\n",
      "Epoch 15/200\n",
      "5435/5435 [==============================] - 3s 552us/step - loss: 0.0070 - acc: 0.9972 - val_loss: 2.7646 - val_acc: 0.6849\n",
      "Epoch 16/200\n",
      "5435/5435 [==============================] - 3s 470us/step - loss: 0.0084 - acc: 0.9974 - val_loss: 2.5615 - val_acc: 0.7165\n",
      "Epoch 17/200\n",
      "5435/5435 [==============================] - 3s 495us/step - loss: 0.0091 - acc: 0.9969 - val_loss: 2.6073 - val_acc: 0.7019\n",
      "Epoch 18/200\n",
      "5435/5435 [==============================] - 3s 571us/step - loss: 0.0071 - acc: 0.9974 - val_loss: 2.7482 - val_acc: 0.7044\n",
      "Epoch 19/200\n",
      "5435/5435 [==============================] - 3s 487us/step - loss: 0.0078 - acc: 0.9976 - val_loss: 2.6823 - val_acc: 0.7105\n",
      "Epoch 20/200\n",
      "5435/5435 [==============================] - 3s 461us/step - loss: 0.0069 - acc: 0.9982 - val_loss: 2.8086 - val_acc: 0.6770\n",
      "Epoch 21/200\n",
      "5435/5435 [==============================] - 2s 458us/step - loss: 0.0051 - acc: 0.9974 - val_loss: 2.7715 - val_acc: 0.7019\n",
      "Epoch 22/200\n",
      "5435/5435 [==============================] - 3s 602us/step - loss: 0.0054 - acc: 0.9983 - val_loss: 2.7441 - val_acc: 0.7038\n",
      "Epoch 23/200\n",
      "5435/5435 [==============================] - 3s 464us/step - loss: 0.0069 - acc: 0.9971 - val_loss: 2.6470 - val_acc: 0.6861\n",
      "Epoch 24/200\n",
      "5435/5435 [==============================] - 3s 473us/step - loss: 0.0105 - acc: 0.9950 - val_loss: 2.6931 - val_acc: 0.7050\n",
      "Epoch 25/200\n",
      "5435/5435 [==============================] - 3s 499us/step - loss: 0.0080 - acc: 0.9967 - val_loss: 2.6400 - val_acc: 0.7062\n",
      "Epoch 26/200\n",
      "5435/5435 [==============================] - 3s 561us/step - loss: 0.0131 - acc: 0.9948 - val_loss: 2.4893 - val_acc: 0.7147\n",
      "Epoch 27/200\n",
      "5435/5435 [==============================] - 3s 470us/step - loss: 0.0127 - acc: 0.9967 - val_loss: 2.6514 - val_acc: 0.7111\n",
      "Epoch 28/200\n",
      "5435/5435 [==============================] - 3s 465us/step - loss: 0.0063 - acc: 0.9978 - val_loss: 2.7595 - val_acc: 0.7086\n",
      "Epoch 29/200\n",
      "5435/5435 [==============================] - 3s 549us/step - loss: 0.0098 - acc: 0.9969 - val_loss: 2.9559 - val_acc: 0.6734\n",
      "Epoch 30/200\n",
      "5435/5435 [==============================] - 3s 513us/step - loss: 0.0110 - acc: 0.9958 - val_loss: 2.7830 - val_acc: 0.7111\n",
      "Epoch 31/200\n",
      "5435/5435 [==============================] - 3s 477us/step - loss: 0.0103 - acc: 0.9965 - val_loss: 2.6698 - val_acc: 0.6940\n",
      "Epoch 32/200\n",
      "5435/5435 [==============================] - 2s 458us/step - loss: 0.0121 - acc: 0.9960 - val_loss: 2.7495 - val_acc: 0.6983\n",
      "Epoch 33/200\n",
      "5435/5435 [==============================] - 3s 594us/step - loss: 0.0155 - acc: 0.9965 - val_loss: 2.7976 - val_acc: 0.6819\n",
      "Epoch 34/200\n",
      "5435/5435 [==============================] - 3s 472us/step - loss: 0.0139 - acc: 0.9952 - val_loss: 2.6635 - val_acc: 0.6898\n",
      "Epoch 35/200\n",
      "5435/5435 [==============================] - 3s 462us/step - loss: 0.0188 - acc: 0.9939 - val_loss: 2.7627 - val_acc: 0.7056\n",
      "Epoch 36/200\n",
      "5435/5435 [==============================] - 3s 517us/step - loss: 0.0176 - acc: 0.9939 - val_loss: 2.8708 - val_acc: 0.7026\n",
      "Epoch 37/200\n",
      "5435/5435 [==============================] - 3s 541us/step - loss: 0.0165 - acc: 0.9954 - val_loss: 2.8341 - val_acc: 0.6934\n",
      "Epoch 38/200\n",
      "5435/5435 [==============================] - 3s 474us/step - loss: 0.0159 - acc: 0.9943 - val_loss: 2.4904 - val_acc: 0.7184\n",
      "Epoch 39/200\n",
      "5435/5435 [==============================] - 2s 458us/step - loss: 0.0183 - acc: 0.9937 - val_loss: 2.5943 - val_acc: 0.7019\n",
      "Epoch 40/200\n",
      "5435/5435 [==============================] - 3s 561us/step - loss: 0.0138 - acc: 0.9948 - val_loss: 2.7510 - val_acc: 0.6922\n",
      "Epoch 41/200\n",
      "5435/5435 [==============================] - 3s 501us/step - loss: 0.0089 - acc: 0.9969 - val_loss: 2.7788 - val_acc: 0.6995\n",
      "Epoch 42/200\n",
      "5435/5435 [==============================] - 3s 493us/step - loss: 0.0122 - acc: 0.9961 - val_loss: 2.6705 - val_acc: 0.6983\n",
      "Epoch 43/200\n",
      "5435/5435 [==============================] - 3s 467us/step - loss: 0.0086 - acc: 0.9960 - val_loss: 2.8494 - val_acc: 0.6989\n",
      "Epoch 44/200\n",
      "5435/5435 [==============================] - 3s 589us/step - loss: 0.0078 - acc: 0.9969 - val_loss: 2.8153 - val_acc: 0.6959\n",
      "Epoch 45/200\n",
      "5435/5435 [==============================] - 2s 460us/step - loss: 0.0094 - acc: 0.9965 - val_loss: 2.7490 - val_acc: 0.6867\n",
      "Epoch 46/200\n",
      "5435/5435 [==============================] - 3s 478us/step - loss: 0.0093 - acc: 0.9965 - val_loss: 2.8628 - val_acc: 0.6904\n",
      "Epoch 47/200\n",
      "5435/5435 [==============================] - 3s 531us/step - loss: 0.0096 - acc: 0.9963 - val_loss: 2.7478 - val_acc: 0.7074\n",
      "Epoch 48/200\n",
      "5435/5435 [==============================] - 3s 534us/step - loss: 0.0092 - acc: 0.9963 - val_loss: 2.7054 - val_acc: 0.7013\n",
      "Epoch 49/200\n",
      "5435/5435 [==============================] - 3s 481us/step - loss: 0.0083 - acc: 0.9965 - val_loss: 2.6984 - val_acc: 0.6995\n",
      "Epoch 50/200\n",
      "5435/5435 [==============================] - 3s 464us/step - loss: 0.0077 - acc: 0.9969 - val_loss: 2.6886 - val_acc: 0.7135\n",
      "Epoch 51/200\n",
      "5435/5435 [==============================] - 3s 600us/step - loss: 0.0079 - acc: 0.9976 - val_loss: 3.0007 - val_acc: 0.6813\n",
      "Epoch 52/200\n",
      "5435/5435 [==============================] - 3s 467us/step - loss: 0.0070 - acc: 0.9963 - val_loss: 2.8948 - val_acc: 0.6953\n",
      "Epoch 53/200\n",
      "5435/5435 [==============================] - 3s 466us/step - loss: 0.0141 - acc: 0.9954 - val_loss: 2.8553 - val_acc: 0.7080\n",
      "Epoch 54/200\n",
      "5435/5435 [==============================] - 3s 521us/step - loss: 0.0128 - acc: 0.9950 - val_loss: 2.9408 - val_acc: 0.6782\n",
      "Epoch 55/200\n",
      "5435/5435 [==============================] - 3s 541us/step - loss: 0.0133 - acc: 0.9948 - val_loss: 2.8530 - val_acc: 0.6880\n",
      "Epoch 56/200\n",
      "5435/5435 [==============================] - 3s 475us/step - loss: 0.0110 - acc: 0.9960 - val_loss: 2.9353 - val_acc: 0.6861\n",
      "Epoch 57/200\n",
      "5435/5435 [==============================] - 3s 484us/step - loss: 0.0082 - acc: 0.9978 - val_loss: 3.0106 - val_acc: 0.6940\n",
      "Epoch 58/200\n",
      "5435/5435 [==============================] - 3s 589us/step - loss: 0.0087 - acc: 0.9967 - val_loss: 2.6307 - val_acc: 0.7123\n",
      "Epoch 59/200\n",
      "5435/5435 [==============================] - 3s 474us/step - loss: 0.0128 - acc: 0.9969 - val_loss: 2.9253 - val_acc: 0.7007\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - 3s 475us/step - loss: 0.0122 - acc: 0.9954 - val_loss: 2.8384 - val_acc: 0.6892\n",
      "Epoch 61/200\n",
      "5435/5435 [==============================] - 3s 485us/step - loss: 0.0114 - acc: 0.9963 - val_loss: 3.0317 - val_acc: 0.6813\n",
      "Epoch 62/200\n",
      "5435/5435 [==============================] - 3s 573us/step - loss: 0.0216 - acc: 0.9932 - val_loss: 2.9175 - val_acc: 0.6788\n",
      "Epoch 63/200\n",
      "5435/5435 [==============================] - 3s 467us/step - loss: 0.0102 - acc: 0.9963 - val_loss: 2.9996 - val_acc: 0.6600\n",
      "Epoch 64/200\n",
      "5435/5435 [==============================] - 3s 479us/step - loss: 0.0133 - acc: 0.9961 - val_loss: 2.9159 - val_acc: 0.6953\n",
      "Epoch 65/200\n",
      "5435/5435 [==============================] - 3s 554us/step - loss: 0.0079 - acc: 0.9961 - val_loss: 2.7473 - val_acc: 0.6825\n",
      "Epoch 66/200\n",
      "5435/5435 [==============================] - 3s 509us/step - loss: 0.0186 - acc: 0.9948 - val_loss: 2.6168 - val_acc: 0.6776\n",
      "Epoch 67/200\n",
      "5435/5435 [==============================] - 3s 495us/step - loss: 0.0162 - acc: 0.9947 - val_loss: 2.6651 - val_acc: 0.6880\n",
      "Epoch 68/200\n",
      "5435/5435 [==============================] - 3s 486us/step - loss: 0.0130 - acc: 0.9958 - val_loss: 2.6831 - val_acc: 0.7202\n",
      "Epoch 69/200\n",
      "5435/5435 [==============================] - 3s 565us/step - loss: 0.0218 - acc: 0.9923 - val_loss: 2.8987 - val_acc: 0.7019\n",
      "Epoch 70/200\n",
      "5435/5435 [==============================] - 3s 461us/step - loss: 0.0071 - acc: 0.9972 - val_loss: 2.6855 - val_acc: 0.6922\n",
      "Epoch 71/200\n",
      "5435/5435 [==============================] - 3s 474us/step - loss: 0.0089 - acc: 0.9969 - val_loss: 2.8766 - val_acc: 0.6867\n",
      "Epoch 72/200\n",
      "5435/5435 [==============================] - 3s 540us/step - loss: 0.0128 - acc: 0.9963 - val_loss: 2.9909 - val_acc: 0.6934\n",
      "Epoch 73/200\n",
      "5435/5435 [==============================] - 3s 538us/step - loss: 0.0121 - acc: 0.9954 - val_loss: 2.9507 - val_acc: 0.6715\n",
      "Epoch 74/200\n",
      "5435/5435 [==============================] - 3s 469us/step - loss: 0.0113 - acc: 0.9960 - val_loss: 2.7415 - val_acc: 0.7111\n",
      "Epoch 75/200\n",
      "5435/5435 [==============================] - 3s 475us/step - loss: 0.0095 - acc: 0.9976 - val_loss: 2.8274 - val_acc: 0.6983\n",
      "Epoch 76/200\n",
      "5435/5435 [==============================] - 3s 592us/step - loss: 0.0257 - acc: 0.9937 - val_loss: 2.8570 - val_acc: 0.6880\n",
      "Epoch 77/200\n",
      "5435/5435 [==============================] - 2s 459us/step - loss: 0.0153 - acc: 0.9960 - val_loss: 2.7329 - val_acc: 0.6892\n",
      "Epoch 78/200\n",
      "5435/5435 [==============================] - 3s 488us/step - loss: 0.0097 - acc: 0.9967 - val_loss: 2.7379 - val_acc: 0.7147\n",
      "Epoch 79/200\n",
      "5435/5435 [==============================] - 3s 505us/step - loss: 0.0081 - acc: 0.9971 - val_loss: 2.7551 - val_acc: 0.6971\n",
      "Epoch 80/200\n",
      "5435/5435 [==============================] - 3s 538us/step - loss: 0.0064 - acc: 0.9978 - val_loss: 2.7390 - val_acc: 0.6940\n",
      "Epoch 81/200\n",
      "5435/5435 [==============================] - 3s 462us/step - loss: 0.0076 - acc: 0.9969 - val_loss: 2.8729 - val_acc: 0.6746\n",
      "Epoch 82/200\n",
      "5435/5435 [==============================] - 3s 477us/step - loss: 0.0092 - acc: 0.9961 - val_loss: 2.9173 - val_acc: 0.6873\n",
      "Epoch 83/200\n",
      "5435/5435 [==============================] - 3s 573us/step - loss: 0.0071 - acc: 0.9972 - val_loss: 2.9134 - val_acc: 0.6989\n",
      "Epoch 84/200\n",
      "5435/5435 [==============================] - 3s 478us/step - loss: 0.0119 - acc: 0.9958 - val_loss: 3.1226 - val_acc: 0.6770\n",
      "Epoch 85/200\n",
      "5435/5435 [==============================] - 3s 488us/step - loss: 0.0122 - acc: 0.9961 - val_loss: 3.0989 - val_acc: 0.6849\n",
      "Epoch 86/200\n",
      "5435/5435 [==============================] - 3s 463us/step - loss: 0.0165 - acc: 0.9943 - val_loss: 2.6727 - val_acc: 0.6983\n",
      "Epoch 87/200\n",
      "5435/5435 [==============================] - 3s 586us/step - loss: 0.0142 - acc: 0.9950 - val_loss: 3.0091 - val_acc: 0.6861\n",
      "Epoch 88/200\n",
      "5435/5435 [==============================] - 3s 461us/step - loss: 0.0110 - acc: 0.9952 - val_loss: 2.4965 - val_acc: 0.7178\n",
      "Epoch 89/200\n",
      "5435/5435 [==============================] - 3s 479us/step - loss: 0.0099 - acc: 0.9967 - val_loss: 2.7915 - val_acc: 0.6934\n",
      "Epoch 90/200\n",
      "5435/5435 [==============================] - 3s 519us/step - loss: 0.0106 - acc: 0.9974 - val_loss: 2.6325 - val_acc: 0.6867\n",
      "Epoch 91/200\n",
      "5435/5435 [==============================] - 3s 528us/step - loss: 0.0083 - acc: 0.9978 - val_loss: 2.8933 - val_acc: 0.6691\n",
      "Epoch 92/200\n",
      "5435/5435 [==============================] - 3s 474us/step - loss: 0.0086 - acc: 0.9967 - val_loss: 3.0881 - val_acc: 0.6655\n",
      "Epoch 93/200\n",
      "5435/5435 [==============================] - 3s 460us/step - loss: 0.0121 - acc: 0.9958 - val_loss: 3.0487 - val_acc: 0.6582\n",
      "Epoch 94/200\n",
      "5435/5435 [==============================] - 3s 589us/step - loss: 0.0280 - acc: 0.9919 - val_loss: 3.0692 - val_acc: 0.6825\n",
      "Epoch 95/200\n",
      "5435/5435 [==============================] - 3s 475us/step - loss: 0.0242 - acc: 0.9930 - val_loss: 2.9881 - val_acc: 0.6758\n",
      "Epoch 96/200\n",
      "5435/5435 [==============================] - 3s 461us/step - loss: 0.0191 - acc: 0.9939 - val_loss: 2.7279 - val_acc: 0.6971\n",
      "Epoch 97/200\n",
      "5435/5435 [==============================] - 3s 473us/step - loss: 0.0184 - acc: 0.9947 - val_loss: 3.1496 - val_acc: 0.6794\n",
      "Epoch 98/200\n",
      "5435/5435 [==============================] - 3s 584us/step - loss: 0.0114 - acc: 0.9963 - val_loss: 2.8559 - val_acc: 0.7013\n",
      "Epoch 99/200\n",
      "5435/5435 [==============================] - 3s 478us/step - loss: 0.0045 - acc: 0.9985 - val_loss: 2.8507 - val_acc: 0.7135\n",
      "Epoch 100/200\n",
      "5435/5435 [==============================] - 3s 475us/step - loss: 0.0096 - acc: 0.9967 - val_loss: 2.7930 - val_acc: 0.7147\n",
      "Epoch 101/200\n",
      "5435/5435 [==============================] - 3s 514us/step - loss: 0.0156 - acc: 0.9950 - val_loss: 2.8436 - val_acc: 0.7092\n",
      "Epoch 102/200\n",
      "5435/5435 [==============================] - 3s 541us/step - loss: 0.0072 - acc: 0.9967 - val_loss: 2.6329 - val_acc: 0.7099\n",
      "Epoch 103/200\n",
      "5435/5435 [==============================] - 3s 478us/step - loss: 0.0109 - acc: 0.9967 - val_loss: 2.6634 - val_acc: 0.7165\n",
      "Epoch 104/200\n",
      "5435/5435 [==============================] - 2s 458us/step - loss: 0.0076 - acc: 0.9969 - val_loss: 2.5916 - val_acc: 0.7281\n",
      "Epoch 105/200\n",
      "5435/5435 [==============================] - 3s 522us/step - loss: 0.0070 - acc: 0.9974 - val_loss: 2.8054 - val_acc: 0.6995\n",
      "Epoch 106/200\n",
      "5435/5435 [==============================] - 3s 524us/step - loss: 0.0108 - acc: 0.9965 - val_loss: 2.9258 - val_acc: 0.6813\n",
      "Epoch 107/200\n",
      "5435/5435 [==============================] - 3s 473us/step - loss: 0.0090 - acc: 0.9972 - val_loss: 2.8592 - val_acc: 0.6989\n",
      "Epoch 108/200\n",
      "5435/5435 [==============================] - 3s 483us/step - loss: 0.0059 - acc: 0.9982 - val_loss: 2.6265 - val_acc: 0.7001\n",
      "Epoch 109/200\n",
      "5435/5435 [==============================] - 3s 568us/step - loss: 0.0066 - acc: 0.9978 - val_loss: 2.5877 - val_acc: 0.7318\n",
      "Epoch 110/200\n",
      "5435/5435 [==============================] - 3s 478us/step - loss: 0.0046 - acc: 0.9983 - val_loss: 2.6712 - val_acc: 0.7196\n",
      "Epoch 111/200\n",
      "5435/5435 [==============================] - 3s 477us/step - loss: 0.0073 - acc: 0.9971 - val_loss: 2.5734 - val_acc: 0.7257\n",
      "Epoch 112/200\n",
      "5435/5435 [==============================] - 3s 494us/step - loss: 0.0069 - acc: 0.9971 - val_loss: 2.6839 - val_acc: 0.7226\n",
      "Epoch 113/200\n",
      "5435/5435 [==============================] - 3s 567us/step - loss: 0.0096 - acc: 0.9963 - val_loss: 2.8327 - val_acc: 0.7068\n",
      "Epoch 114/200\n",
      "5435/5435 [==============================] - 2s 458us/step - loss: 0.0078 - acc: 0.9982 - val_loss: 2.8466 - val_acc: 0.6934\n",
      "Epoch 115/200\n",
      "5435/5435 [==============================] - 3s 473us/step - loss: 0.0046 - acc: 0.9985 - val_loss: 2.9142 - val_acc: 0.7050\n",
      "Epoch 116/200\n",
      "5435/5435 [==============================] - 3s 529us/step - loss: 0.0049 - acc: 0.9982 - val_loss: 2.9013 - val_acc: 0.7117\n",
      "Epoch 117/200\n",
      "5435/5435 [==============================] - 3s 531us/step - loss: 0.0054 - acc: 0.9982 - val_loss: 3.0319 - val_acc: 0.6867\n",
      "Epoch 118/200\n",
      "5435/5435 [==============================] - 3s 475us/step - loss: 0.0077 - acc: 0.9974 - val_loss: 3.2824 - val_acc: 0.6746\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - 3s 469us/step - loss: 0.0045 - acc: 0.9974 - val_loss: 2.7951 - val_acc: 0.7062\n",
      "Epoch 120/200\n",
      "5435/5435 [==============================] - 3s 601us/step - loss: 0.0078 - acc: 0.9967 - val_loss: 2.8730 - val_acc: 0.6904\n",
      "Epoch 121/200\n",
      "5435/5435 [==============================] - 3s 470us/step - loss: 0.0060 - acc: 0.9982 - val_loss: 2.9076 - val_acc: 0.6989\n",
      "Epoch 122/200\n",
      "5435/5435 [==============================] - 3s 484us/step - loss: 0.0065 - acc: 0.9978 - val_loss: 2.9096 - val_acc: 0.7153\n",
      "Epoch 123/200\n",
      "5435/5435 [==============================] - 3s 487us/step - loss: 0.0099 - acc: 0.9969 - val_loss: 2.8271 - val_acc: 0.6946\n",
      "Epoch 124/200\n",
      "5435/5435 [==============================] - 3s 577us/step - loss: 0.0095 - acc: 0.9976 - val_loss: 2.7638 - val_acc: 0.7038\n",
      "Epoch 125/200\n",
      "5435/5435 [==============================] - 3s 463us/step - loss: 0.0097 - acc: 0.9971 - val_loss: 2.6145 - val_acc: 0.7019\n",
      "Epoch 126/200\n",
      "5435/5435 [==============================] - 3s 490us/step - loss: 0.0110 - acc: 0.9971 - val_loss: 2.5951 - val_acc: 0.7172\n",
      "Epoch 127/200\n",
      "5435/5435 [==============================] - 3s 541us/step - loss: 0.0141 - acc: 0.9958 - val_loss: 2.7198 - val_acc: 0.7184\n",
      "Epoch 128/200\n",
      "5435/5435 [==============================] - 3s 508us/step - loss: 0.0239 - acc: 0.9932 - val_loss: 2.9558 - val_acc: 0.6946\n",
      "Epoch 129/200\n",
      "5435/5435 [==============================] - 2s 459us/step - loss: 0.0191 - acc: 0.9941 - val_loss: 2.8288 - val_acc: 0.7147\n",
      "Epoch 130/200\n",
      "5435/5435 [==============================] - 3s 478us/step - loss: 0.0155 - acc: 0.9952 - val_loss: 2.7303 - val_acc: 0.7038\n",
      "Epoch 131/200\n",
      "5435/5435 [==============================] - 3s 593us/step - loss: 0.0186 - acc: 0.9952 - val_loss: 2.7841 - val_acc: 0.7178\n",
      "Epoch 132/200\n",
      "5435/5435 [==============================] - 3s 462us/step - loss: 0.0117 - acc: 0.9965 - val_loss: 2.7962 - val_acc: 0.6965\n",
      "Epoch 133/200\n",
      "5435/5435 [==============================] - 3s 477us/step - loss: 0.0132 - acc: 0.9971 - val_loss: 2.8211 - val_acc: 0.7001\n",
      "Epoch 134/200\n",
      "5435/5435 [==============================] - 3s 482us/step - loss: 0.0123 - acc: 0.9954 - val_loss: 2.7591 - val_acc: 0.7105\n",
      "Epoch 135/200\n",
      "5435/5435 [==============================] - 3s 578us/step - loss: 0.0090 - acc: 0.9969 - val_loss: 2.8998 - val_acc: 0.6995\n",
      "Epoch 136/200\n",
      "5435/5435 [==============================] - 3s 477us/step - loss: 0.0164 - acc: 0.9954 - val_loss: 3.0205 - val_acc: 0.6673\n",
      "Epoch 137/200\n",
      "5435/5435 [==============================] - 3s 461us/step - loss: 0.0149 - acc: 0.9963 - val_loss: 2.9385 - val_acc: 0.7013\n",
      "Epoch 138/200\n",
      "5435/5435 [==============================] - 3s 536us/step - loss: 0.0087 - acc: 0.9976 - val_loss: 3.0486 - val_acc: 0.6861\n",
      "Epoch 139/200\n",
      "5435/5435 [==============================] - 3s 513us/step - loss: 0.0104 - acc: 0.9961 - val_loss: 2.8846 - val_acc: 0.6916\n",
      "Epoch 140/200\n",
      "5435/5435 [==============================] - 3s 492us/step - loss: 0.0142 - acc: 0.9956 - val_loss: 2.7380 - val_acc: 0.7129\n",
      "Epoch 141/200\n",
      "5435/5435 [==============================] - 3s 461us/step - loss: 0.0142 - acc: 0.9967 - val_loss: 2.8902 - val_acc: 0.6916\n",
      "Epoch 142/200\n",
      "5435/5435 [==============================] - 3s 591us/step - loss: 0.0091 - acc: 0.9965 - val_loss: 2.8531 - val_acc: 0.7062\n",
      "Epoch 143/200\n",
      "5435/5435 [==============================] - 3s 470us/step - loss: 0.0123 - acc: 0.9956 - val_loss: 3.2816 - val_acc: 0.6442\n",
      "Epoch 144/200\n",
      "5435/5435 [==============================] - 3s 477us/step - loss: 0.0222 - acc: 0.9921 - val_loss: 3.2022 - val_acc: 0.6758\n",
      "Epoch 145/200\n",
      "5435/5435 [==============================] - 3s 530us/step - loss: 0.0162 - acc: 0.9950 - val_loss: 3.1035 - val_acc: 0.6758\n",
      "Epoch 146/200\n",
      "5435/5435 [==============================] - 3s 539us/step - loss: 0.0114 - acc: 0.9963 - val_loss: 3.1446 - val_acc: 0.6740\n",
      "Epoch 147/200\n",
      "5435/5435 [==============================] - 3s 476us/step - loss: 0.0102 - acc: 0.9965 - val_loss: 3.0141 - val_acc: 0.6934\n",
      "Epoch 148/200\n",
      "5435/5435 [==============================] - 3s 463us/step - loss: 0.0065 - acc: 0.9972 - val_loss: 3.2449 - val_acc: 0.6910\n",
      "Epoch 149/200\n",
      "5435/5435 [==============================] - 3s 605us/step - loss: 0.0030 - acc: 0.9985 - val_loss: 2.9852 - val_acc: 0.6965\n",
      "Epoch 150/200\n",
      "5435/5435 [==============================] - 3s 479us/step - loss: 0.0066 - acc: 0.9978 - val_loss: 3.1550 - val_acc: 0.6758\n",
      "Epoch 151/200\n",
      "5435/5435 [==============================] - 3s 492us/step - loss: 0.0056 - acc: 0.9982 - val_loss: 3.1185 - val_acc: 0.6916\n",
      "Epoch 152/200\n",
      "5435/5435 [==============================] - 3s 528us/step - loss: 0.0071 - acc: 0.9974 - val_loss: 2.7094 - val_acc: 0.7105\n",
      "Epoch 153/200\n",
      "5435/5435 [==============================] - 3s 551us/step - loss: 0.0063 - acc: 0.9978 - val_loss: 2.8737 - val_acc: 0.7214\n",
      "Epoch 154/200\n",
      "5435/5435 [==============================] - 3s 466us/step - loss: 0.0066 - acc: 0.9980 - val_loss: 2.7236 - val_acc: 0.7129\n",
      "Epoch 155/200\n",
      "5435/5435 [==============================] - 3s 488us/step - loss: 0.0086 - acc: 0.9969 - val_loss: 2.6605 - val_acc: 0.7330\n",
      "Epoch 156/200\n",
      "5435/5435 [==============================] - 3s 575us/step - loss: 0.0093 - acc: 0.9976 - val_loss: 2.6328 - val_acc: 0.7281\n",
      "Epoch 157/200\n",
      "5435/5435 [==============================] - 3s 486us/step - loss: 0.0077 - acc: 0.9971 - val_loss: 2.5482 - val_acc: 0.7409\n",
      "Epoch 158/200\n",
      "5435/5435 [==============================] - 3s 469us/step - loss: 0.0070 - acc: 0.9976 - val_loss: 2.7399 - val_acc: 0.7056\n",
      "Epoch 159/200\n",
      "5435/5435 [==============================] - 3s 512us/step - loss: 0.0116 - acc: 0.9967 - val_loss: 2.7483 - val_acc: 0.7141\n",
      "Epoch 160/200\n",
      "5435/5435 [==============================] - 3s 546us/step - loss: 0.0096 - acc: 0.9971 - val_loss: 2.7992 - val_acc: 0.6849\n",
      "Epoch 161/200\n",
      "5435/5435 [==============================] - 3s 463us/step - loss: 0.0148 - acc: 0.9969 - val_loss: 3.0324 - val_acc: 0.6910\n",
      "Epoch 162/200\n",
      "5435/5435 [==============================] - 3s 494us/step - loss: 0.0117 - acc: 0.9960 - val_loss: 3.1545 - val_acc: 0.6673\n",
      "Epoch 163/200\n",
      "5435/5435 [==============================] - 3s 552us/step - loss: 0.0074 - acc: 0.9978 - val_loss: 2.8180 - val_acc: 0.7080\n",
      "Epoch 164/200\n",
      "5435/5435 [==============================] - 3s 510us/step - loss: 0.0027 - acc: 0.9987 - val_loss: 2.8977 - val_acc: 0.7092\n",
      "Epoch 165/200\n",
      "5435/5435 [==============================] - 2s 459us/step - loss: 0.0080 - acc: 0.9971 - val_loss: 3.1120 - val_acc: 0.6697\n",
      "Epoch 166/200\n",
      "5435/5435 [==============================] - 3s 509us/step - loss: 0.0115 - acc: 0.9969 - val_loss: 3.1205 - val_acc: 0.6703\n",
      "Epoch 167/200\n",
      "5435/5435 [==============================] - 3s 552us/step - loss: 0.0081 - acc: 0.9974 - val_loss: 2.9097 - val_acc: 0.7062\n",
      "Epoch 168/200\n",
      "5435/5435 [==============================] - 3s 470us/step - loss: 0.0064 - acc: 0.9969 - val_loss: 2.9982 - val_acc: 0.6873\n",
      "Epoch 169/200\n",
      "5435/5435 [==============================] - 3s 483us/step - loss: 0.0052 - acc: 0.9976 - val_loss: 3.0483 - val_acc: 0.6867\n",
      "Epoch 170/200\n",
      "5435/5435 [==============================] - 3s 542us/step - loss: 0.0110 - acc: 0.9971 - val_loss: 3.2044 - val_acc: 0.6703\n",
      "Epoch 171/200\n",
      "5435/5435 [==============================] - 3s 524us/step - loss: 0.0101 - acc: 0.9974 - val_loss: 2.9282 - val_acc: 0.7068\n",
      "Epoch 172/200\n",
      "5435/5435 [==============================] - 3s 482us/step - loss: 0.0097 - acc: 0.9971 - val_loss: 3.0061 - val_acc: 0.7026\n",
      "Epoch 173/200\n",
      "5435/5435 [==============================] - 3s 476us/step - loss: 0.0183 - acc: 0.9947 - val_loss: 3.3875 - val_acc: 0.6685\n",
      "Epoch 174/200\n",
      "5435/5435 [==============================] - 3s 573us/step - loss: 0.0069 - acc: 0.9972 - val_loss: 2.9685 - val_acc: 0.6940\n",
      "Epoch 175/200\n",
      "5435/5435 [==============================] - 3s 494us/step - loss: 0.0105 - acc: 0.9967 - val_loss: 3.1010 - val_acc: 0.6800\n",
      "Epoch 176/200\n",
      "5435/5435 [==============================] - 3s 461us/step - loss: 0.0151 - acc: 0.9958 - val_loss: 3.2952 - val_acc: 0.6794\n",
      "Epoch 177/200\n",
      "5435/5435 [==============================] - 3s 547us/step - loss: 0.0116 - acc: 0.9967 - val_loss: 3.1499 - val_acc: 0.6995\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - 3s 517us/step - loss: 0.0069 - acc: 0.9974 - val_loss: 3.1088 - val_acc: 0.6928\n",
      "Epoch 179/200\n",
      "5435/5435 [==============================] - 3s 478us/step - loss: 0.0112 - acc: 0.9974 - val_loss: 3.2520 - val_acc: 0.7001\n",
      "Epoch 180/200\n",
      "5435/5435 [==============================] - 3s 488us/step - loss: 0.0068 - acc: 0.9980 - val_loss: 3.2052 - val_acc: 0.6959\n",
      "Epoch 181/200\n",
      "5435/5435 [==============================] - 3s 575us/step - loss: 0.0093 - acc: 0.9971 - val_loss: 3.0067 - val_acc: 0.7050\n",
      "Epoch 182/200\n",
      "5435/5435 [==============================] - 3s 486us/step - loss: 0.0041 - acc: 0.9989 - val_loss: 2.8524 - val_acc: 0.7080\n",
      "Epoch 183/200\n",
      "5435/5435 [==============================] - 3s 472us/step - loss: 0.0062 - acc: 0.9972 - val_loss: 2.9501 - val_acc: 0.7141\n",
      "Epoch 184/200\n",
      "5435/5435 [==============================] - 3s 495us/step - loss: 0.0077 - acc: 0.9982 - val_loss: 2.8745 - val_acc: 0.7123\n",
      "Epoch 185/200\n",
      "5435/5435 [==============================] - 3s 565us/step - loss: 0.0049 - acc: 0.9982 - val_loss: 2.7962 - val_acc: 0.7153\n",
      "Epoch 186/200\n",
      "5435/5435 [==============================] - 3s 481us/step - loss: 0.0056 - acc: 0.9980 - val_loss: 3.1935 - val_acc: 0.6679\n",
      "Epoch 187/200\n",
      "5435/5435 [==============================] - 2s 458us/step - loss: 0.0107 - acc: 0.9960 - val_loss: 3.0663 - val_acc: 0.6855\n",
      "Epoch 188/200\n",
      "5435/5435 [==============================] - 3s 524us/step - loss: 0.0214 - acc: 0.9939 - val_loss: 2.8466 - val_acc: 0.7062\n",
      "Epoch 189/200\n",
      "5435/5435 [==============================] - 3s 539us/step - loss: 0.0129 - acc: 0.9956 - val_loss: 2.7724 - val_acc: 0.7208\n",
      "Epoch 190/200\n",
      "5435/5435 [==============================] - 2s 460us/step - loss: 0.0122 - acc: 0.9963 - val_loss: 2.8966 - val_acc: 0.7038\n",
      "Epoch 191/200\n",
      "5435/5435 [==============================] - 3s 484us/step - loss: 0.0103 - acc: 0.9972 - val_loss: 2.9150 - val_acc: 0.7135\n",
      "Epoch 192/200\n",
      "5435/5435 [==============================] - 3s 577us/step - loss: 0.0085 - acc: 0.9974 - val_loss: 2.8599 - val_acc: 0.6989\n",
      "Epoch 193/200\n",
      "5435/5435 [==============================] - 3s 474us/step - loss: 0.0061 - acc: 0.9978 - val_loss: 3.0939 - val_acc: 0.7038\n",
      "Epoch 194/200\n",
      "5435/5435 [==============================] - 2s 457us/step - loss: 0.0175 - acc: 0.9950 - val_loss: 2.7903 - val_acc: 0.7263\n",
      "Epoch 195/200\n",
      "5435/5435 [==============================] - 3s 516us/step - loss: 0.0175 - acc: 0.9958 - val_loss: 3.4647 - val_acc: 0.6673\n",
      "Epoch 196/200\n",
      "5435/5435 [==============================] - 3s 546us/step - loss: 0.0124 - acc: 0.9961 - val_loss: 3.0978 - val_acc: 0.6928\n",
      "Epoch 197/200\n",
      "5435/5435 [==============================] - 3s 466us/step - loss: 0.0123 - acc: 0.9965 - val_loss: 3.2455 - val_acc: 0.6734\n",
      "Epoch 198/200\n",
      "5435/5435 [==============================] - 3s 472us/step - loss: 0.0129 - acc: 0.9969 - val_loss: 3.1604 - val_acc: 0.6861\n",
      "Epoch 199/200\n",
      "5435/5435 [==============================] - 3s 570us/step - loss: 0.0069 - acc: 0.9980 - val_loss: 3.1523 - val_acc: 0.6959\n",
      "Epoch 200/200\n",
      "5435/5435 [==============================] - 3s 503us/step - loss: 0.0044 - acc: 0.9983 - val_loss: 3.0493 - val_acc: 0.7013\n"
     ]
    }
   ],
   "source": [
    "history = m.fit(x=X_train, y=y_train, batch_size=130,\n",
    "                    epochs=200, verbose=True,\n",
    "                    validation_split=0.0,\n",
    "                    validation_data=(X_val, y_val), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn(n_features=n_features, n_frames=n_frames, n_filters_cnn=64,\n",
    "                     n_classes=10):\n",
    "\n",
    "    # INPUT\n",
    "    x = Input(shape=(n_features, n_frames), dtype='float32')\n",
    "  \n",
    "    y= LSTM(256, return_sequences = True)(x)\n",
    "    #y= Dropout(0.2)(y)  \n",
    "    \n",
    "    #y= LSTM(256)(y)\n",
    "    #y= Dropout(0.2)(y) \n",
    "\n",
    "    # Flatten for dense layers\n",
    "    y = Flatten()(y)\n",
    "    #y = Dropout(0.5)(y)\n",
    "    #y = Dense(n_dense_cnn, activation='relu')(y)\n",
    "\n",
    "    #y = Dropout(0.5)(y)\n",
    "    y = Dense(n_classes, activation='sigmoid')(y)\n",
    "\n",
    "    m = Model(inputs=x, outputs=y)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer lstm_1: expected ndim=3, found ndim=4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-f5c54bc906c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-179-5e9628b1a540>\u001b[0m in \u001b[0;36mbuild_rnn\u001b[0;34m(n_features, n_frames, n_filters_cnn, n_classes)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m#y= Dropout(0.2)(y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/user/cvsspstf/is0017/DCASE_baseline/local/lib/python2.7/site-packages/keras/layers/recurrent.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/user/cvsspstf/is0017/DCASE_baseline/local/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/user/cvsspstf/is0017/DCASE_baseline/local/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    456\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer lstm_1: expected ndim=3, found ndim=4"
     ]
    }
   ],
   "source": [
    "m = build_rnn(n_features=40,)\n",
    "m.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
