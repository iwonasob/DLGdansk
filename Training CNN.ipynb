{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stacked Convolutional and Recurrent Neural Networks for Audio Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img style=\"float:center\" width=200 src=\"figures/CRNN_tampere.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# FIX ALL THE RANDOM VALUES TO GET REPRODUCIBLE RESULTS\n",
    "import os\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "\n",
    "import random as rn\n",
    "rn.seed(1254)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1515)\n",
    "\n",
    "# SET NICE PLOTTING\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(2)\n",
    "\n",
    "config = tf.ConfigProto(\n",
    "    intra_op_parallelism_threads=1,\n",
    "    inter_op_parallelism_threads=1 \n",
    ")\n",
    "\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "sess = tf.Session(graph= tf.get_default_graph(), config=config) \n",
    "    \n",
    "import keras \n",
    "from keras import backend as K\n",
    "K.set_session(sess)  # set this TensorFlow session as the default session for Keras\n",
    "\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#SET PATHS\n",
    "workspace = '/home/speakers/isobieraj/workspace'\n",
    "\n",
    "hdf5_path = os.path.join(workspace,'dataset.hdf5') \n",
    "scalerpath = os.path.join(workspace,'scaler.pkl')\n",
    "\n",
    "modelfolder = os.path.join(workspace,'models')\n",
    "if not os.path.isdir(modelfolder):\n",
    "    os.makedirs(modelfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "hf = h5py.File(hdf5_path, 'r')\n",
    "X_train = np.array(hf.get('X_train') )\n",
    "y_train = np.array(hf.get('y_train'))\n",
    "X_val =  np.array(hf.get('X_val'))\n",
    "y_val= np.array(hf.get('y_val'))\n",
    "X_test = np.array(hf.get('X_test'))\n",
    "y_test = np.array(hf.get('y_test'))\n",
    "\n",
    "n_features= hf.get('n_features').value\n",
    "n_frames=hf.get('max_length_samp').value\n",
    "label_list= hf.get('label_list').value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# SCALE THE DATA\n",
    "with open(scalerpath, 'rb') as f:\n",
    "    scaler = pickle.load(f, encoding='latin1') \n",
    "\n",
    "#scaler=pickle.load(open(scalerpath, 'rb'))\n",
    "\n",
    "X_train_scaled = [scaler.transform(x.T) for x in X_train]\n",
    "X_val_scaled = [scaler.transform(x.T) for x in X_val]\n",
    "X_test_scaled = [scaler.transform(x.T) for x in X_test]\n",
    "\n",
    "# CHANGE DIMENSION TO FIT KERAS\n",
    "X_train = np.expand_dims(X_train_scaled, 3)\n",
    "X_val = np.expand_dims(X_val_scaled, 3)\n",
    "X_test = np.expand_dims(X_test_scaled, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def build_cnn(n_frames=n_frames, n_features=n_features,  n_filters_cnn=48,\n",
    "                     filter_size_cnn=(3, 3), pool_size=(2,2),\n",
    "                     n_classes=10):\n",
    "\n",
    "    # INPUT\n",
    "    x = Input(shape=(n_frames, n_features,  1), dtype='float32')\n",
    "\n",
    "    # CONV 1\n",
    "    y = Conv2D(n_filters_cnn, filter_size_cnn, padding='valid', \n",
    "               kernel_regularizer=l2(0.01), activation='relu')(x)\n",
    "    y = MaxPooling2D(pool_size=pool_size, strides=None, padding='valid')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    \n",
    "    y = Dropout(0.5)(y)\n",
    "    # CONV 2\n",
    "    y = Conv2D(n_filters_cnn, filter_size_cnn, padding='valid',\n",
    "               kernel_regularizer=l2(0.01), activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=pool_size, strides=None, padding='valid')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    \n",
    "    y = Dropout(0.5)(y)\n",
    "    # CONV 3\n",
    "    y = Conv2D(n_filters_cnn, filter_size_cnn, padding='valid',\n",
    "               kernel_regularizer=l2(0.01), activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=pool_size, strides=None, padding='valid')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "\n",
    "    # Flatten for dense layers\n",
    "    y = Flatten()(y)\n",
    "\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Dense(n_classes, activation='softmax')(y)\n",
    "\n",
    "    m = Model(inputs=x, outputs=y)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 173, 40, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 171, 38, 48)       480       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 85, 19, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 85, 19, 48)        192       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 85, 19, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 83, 17, 48)        20784     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 41, 8, 48)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 41, 8, 48)         192       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 41, 8, 48)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 39, 6, 48)         20784     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 19, 3, 48)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 19, 3, 48)         192       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2736)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2736)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                27370     \n",
      "=================================================================\n",
      "Total params: 69,994\n",
      "Trainable params: 69,706\n",
      "Non-trainable params: 288\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m = build_cnn(n_features=40,)\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(m, to_file='figures/cnn_model_shape.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img width = 300 src='figures/cnn_model_shape.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "adam= keras.optimizers.Adam(lr=0.001, decay=1e-5)\n",
    "m.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(\n",
    "                 os.path.join(modelfolder, \n",
    "                        'cnn_epoch_{epoch:03d}_val_loss_{val_loss:.4f}.hdf5'),\n",
    "                 monitor='val_loss', \n",
    "                 save_best_only=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, verbose=1)\n",
    "callbacks = [early_stopping, model_checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7079 samples, validate on 816 samples\n",
      "Epoch 1/200\n",
      "7079/7079 [==============================] - 182s 26ms/step - loss: 3.1130 - acc: 0.3204 - val_loss: 3.8875 - val_acc: 0.2537\n",
      "Epoch 2/200\n",
      "7079/7079 [==============================] - 180s 25ms/step - loss: 2.6298 - acc: 0.4484 - val_loss: 3.4442 - val_acc: 0.3333\n",
      "Epoch 3/200\n",
      "7079/7079 [==============================] - 176s 25ms/step - loss: 2.1166 - acc: 0.5756 - val_loss: 2.8337 - val_acc: 0.3873\n",
      "Epoch 5/200\n",
      "7079/7079 [==============================] - 177s 25ms/step - loss: 1.9322 - acc: 0.6114 - val_loss: 2.6875 - val_acc: 0.4167\n",
      "Epoch 6/200\n",
      "7079/7079 [==============================] - 176s 25ms/step - loss: 1.7775 - acc: 0.6543 - val_loss: 2.7730 - val_acc: 0.4314\n",
      "Epoch 7/200\n",
      "7079/7079 [==============================] - 176s 25ms/step - loss: 1.6250 - acc: 0.6816 - val_loss: 2.4600 - val_acc: 0.4890\n",
      "Epoch 8/200\n",
      "7079/7079 [==============================] - 177s 25ms/step - loss: 1.5034 - acc: 0.7111 - val_loss: 2.2469 - val_acc: 0.5319\n",
      "Epoch 9/200\n",
      "7079/7079 [==============================] - 175s 25ms/step - loss: 1.4057 - acc: 0.7226 - val_loss: 2.2998 - val_acc: 0.5319\n",
      "Epoch 10/200\n",
      "7079/7079 [==============================] - 174s 25ms/step - loss: 1.3492 - acc: 0.7354 - val_loss: 2.5193 - val_acc: 0.4902\n",
      "Epoch 11/200\n",
      "7079/7079 [==============================] - 174s 25ms/step - loss: 1.1413 - acc: 0.7751 - val_loss: 1.8797 - val_acc: 0.5858\n",
      "Epoch 13/200\n",
      "7079/7079 [==============================] - 174s 25ms/step - loss: 1.0530 - acc: 0.7946 - val_loss: 1.8861 - val_acc: 0.5772\n",
      "Epoch 14/200\n",
      "7079/7079 [==============================] - 175s 25ms/step - loss: 1.0110 - acc: 0.7995 - val_loss: 1.9870 - val_acc: 0.5686\n",
      "Epoch 15/200\n",
      "7079/7079 [==============================] - 174s 25ms/step - loss: 0.9515 - acc: 0.8142 - val_loss: 1.8904 - val_acc: 0.5846\n",
      "Epoch 16/200\n",
      "7079/7079 [==============================] - 173s 24ms/step - loss: 0.8707 - acc: 0.8248 - val_loss: 2.2130 - val_acc: 0.5355\n",
      "Epoch 17/200\n",
      "7079/7079 [==============================] - 172s 24ms/step - loss: 0.8368 - acc: 0.8309 - val_loss: 1.9425 - val_acc: 0.6017\n",
      "Epoch 18/200\n",
      "7079/7079 [==============================] - 167s 24ms/step - loss: 0.7925 - acc: 0.8387 - val_loss: 2.1049 - val_acc: 0.5625\n",
      "Epoch 19/200\n",
      "7079/7079 [==============================] - 167s 24ms/step - loss: 0.7461 - acc: 0.8486 - val_loss: 1.9121 - val_acc: 0.5993\n",
      "Epoch 20/200\n",
      "7079/7079 [==============================] - 165s 23ms/step - loss: 0.7029 - acc: 0.8600 - val_loss: 2.3134 - val_acc: 0.5515\n",
      "Epoch 21/200\n",
      "7079/7079 [==============================] - 172s 24ms/step - loss: 0.6790 - acc: 0.8590 - val_loss: 1.9839 - val_acc: 0.5662\n",
      "Epoch 22/200\n",
      "7079/7079 [==============================] - 171s 24ms/step - loss: 0.6439 - acc: 0.8681 - val_loss: 2.1371 - val_acc: 0.5723\n",
      "Epoch 23/200\n",
      "7079/7079 [==============================] - 174s 25ms/step - loss: 0.6365 - acc: 0.8671 - val_loss: 1.8373 - val_acc: 0.5870\n",
      "Epoch 24/200\n",
      "7079/7079 [==============================] - 170s 24ms/step - loss: 0.5644 - acc: 0.8835 - val_loss: 1.9757 - val_acc: 0.5993\n",
      "Epoch 25/200\n",
      "7079/7079 [==============================] - 177s 25ms/step - loss: 0.5632 - acc: 0.8864 - val_loss: 2.1628 - val_acc: 0.5944\n",
      "Epoch 26/200\n",
      "7079/7079 [==============================] - 173s 24ms/step - loss: 0.5322 - acc: 0.8905 - val_loss: 1.9432 - val_acc: 0.5919\n",
      "Epoch 27/200\n",
      "7079/7079 [==============================] - 167s 24ms/step - loss: 0.5079 - acc: 0.8907 - val_loss: 2.0067 - val_acc: 0.5882\n",
      "Epoch 28/200\n",
      "7079/7079 [==============================] - 174s 25ms/step - loss: 0.4900 - acc: 0.9003 - val_loss: 2.2401 - val_acc: 0.6042\n",
      "Epoch 29/200\n",
      "7079/7079 [==============================] - 170s 24ms/step - loss: 0.4651 - acc: 0.9078 - val_loss: 2.2554 - val_acc: 0.5968\n",
      "Epoch 30/200\n",
      "7079/7079 [==============================] - 170s 24ms/step - loss: 0.4591 - acc: 0.9022 - val_loss: 2.2896 - val_acc: 0.5650\n",
      "Epoch 31/200\n",
      "7079/7079 [==============================] - 169s 24ms/step - loss: 0.4468 - acc: 0.9007 - val_loss: 1.9212 - val_acc: 0.6667\n",
      "Epoch 32/200\n",
      "7079/7079 [==============================] - 169s 24ms/step - loss: 0.4263 - acc: 0.9119 - val_loss: 1.9947 - val_acc: 0.6213\n",
      "Epoch 33/200\n",
      "7079/7079 [==============================] - 168s 24ms/step - loss: 0.4170 - acc: 0.9124 - val_loss: 2.2011 - val_acc: 0.6213\n",
      "Epoch 34/200\n",
      "7079/7079 [==============================] - 167s 24ms/step - loss: 0.4103 - acc: 0.9134 - val_loss: 2.0821 - val_acc: 0.6176\n",
      "Epoch 35/200\n",
      "7079/7079 [==============================] - 168s 24ms/step - loss: 0.4050 - acc: 0.9134 - val_loss: 1.8137 - val_acc: 0.6728\n",
      "Epoch 36/200\n",
      "7079/7079 [==============================] - 168s 24ms/step - loss: 0.3810 - acc: 0.9212 - val_loss: 1.8649 - val_acc: 0.6728\n",
      "Epoch 37/200\n",
      "7079/7079 [==============================] - 167s 24ms/step - loss: 0.3909 - acc: 0.9119 - val_loss: 1.6689 - val_acc: 0.6618\n",
      "Epoch 38/200\n",
      "7079/7079 [==============================] - 166s 23ms/step - loss: 0.3871 - acc: 0.9151 - val_loss: 1.9911 - val_acc: 0.6446\n",
      "Epoch 39/200\n",
      "7079/7079 [==============================] - 166s 23ms/step - loss: 0.3706 - acc: 0.9213 - val_loss: 1.8975 - val_acc: 0.6544\n",
      "Epoch 40/200\n",
      "7079/7079 [==============================] - 167s 24ms/step - loss: 0.3567 - acc: 0.9260 - val_loss: 1.7631 - val_acc: 0.6961\n",
      "Epoch 41/200\n",
      "7079/7079 [==============================] - 241s 34ms/step - loss: 0.3425 - acc: 0.9299 - val_loss: 1.8980 - val_acc: 0.6311\n",
      "Epoch 42/200\n",
      "7079/7079 [==============================] - 259s 37ms/step - loss: 0.3534 - acc: 0.9246 - val_loss: 1.7905 - val_acc: 0.6532\n",
      "Epoch 43/200\n",
      "7079/7079 [==============================] - 262s 37ms/step - loss: 0.3404 - acc: 0.9294 - val_loss: 2.0595 - val_acc: 0.6630\n",
      "Epoch 44/200\n",
      "7079/7079 [==============================] - 259s 37ms/step - loss: 0.3453 - acc: 0.9272 - val_loss: 1.8895 - val_acc: 0.6826\n",
      "Epoch 45/200\n",
      "7079/7079 [==============================] - 255s 36ms/step - loss: 0.3622 - acc: 0.9217 - val_loss: 1.9668 - val_acc: 0.6299\n",
      "Epoch 46/200\n",
      "7079/7079 [==============================] - 285s 40ms/step - loss: 0.3519 - acc: 0.9236 - val_loss: 2.1232 - val_acc: 0.6434\n",
      "Epoch 47/200\n",
      "7079/7079 [==============================] - 210s 30ms/step - loss: 0.3173 - acc: 0.9376 - val_loss: 1.8077 - val_acc: 0.6556\n",
      "Epoch 48/200\n",
      "7079/7079 [==============================] - 238s 34ms/step - loss: 0.3313 - acc: 0.9315 - val_loss: 1.5887 - val_acc: 0.7341\n",
      "Epoch 49/200\n",
      "7079/7079 [==============================] - 267s 38ms/step - loss: 0.3293 - acc: 0.9335 - val_loss: 1.8301 - val_acc: 0.6863\n",
      "Epoch 50/200\n",
      "7079/7079 [==============================] - 281s 40ms/step - loss: 0.3206 - acc: 0.9335 - val_loss: 1.8940 - val_acc: 0.7071\n",
      "Epoch 51/200\n",
      "7079/7079 [==============================] - 363s 51ms/step - loss: 0.3076 - acc: 0.9410 - val_loss: 2.0187 - val_acc: 0.6875\n",
      "Epoch 52/200\n",
      "7079/7079 [==============================] - 380s 54ms/step - loss: 0.3122 - acc: 0.9364 - val_loss: 2.0281 - val_acc: 0.6801\n",
      "Epoch 53/200\n",
      "7079/7079 [==============================] - 446s 63ms/step - loss: 0.3135 - acc: 0.9381 - val_loss: 1.7804 - val_acc: 0.6924\n",
      "Epoch 54/200\n",
      "7079/7079 [==============================] - 449s 63ms/step - loss: 0.3104 - acc: 0.9339 - val_loss: 1.8373 - val_acc: 0.6691\n",
      "Epoch 55/200\n",
      "7079/7079 [==============================] - 332s 47ms/step - loss: 0.3218 - acc: 0.9343 - val_loss: 1.7018 - val_acc: 0.7194\n",
      "Epoch 56/200\n",
      "7079/7079 [==============================] - 284s 40ms/step - loss: 0.3077 - acc: 0.9350 - val_loss: 1.8643 - val_acc: 0.6949\n",
      "Epoch 57/200\n",
      "7079/7079 [==============================] - 291s 41ms/step - loss: 0.3029 - acc: 0.9419 - val_loss: 1.8408 - val_acc: 0.6924\n",
      "Epoch 58/200\n",
      "7079/7079 [==============================] - 284s 40ms/step - loss: 0.2903 - acc: 0.9446 - val_loss: 1.6967 - val_acc: 0.7390\n",
      "Epoch 59/200\n",
      "7079/7079 [==============================] - 273s 39ms/step - loss: 0.3017 - acc: 0.9400 - val_loss: 1.5873 - val_acc: 0.7341\n",
      "Epoch 60/200\n",
      "7079/7079 [==============================] - 270s 38ms/step - loss: 0.2744 - acc: 0.9473 - val_loss: 1.8587 - val_acc: 0.7034\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7079/7079 [==============================] - 268s 38ms/step - loss: 0.2750 - acc: 0.9480 - val_loss: 1.8068 - val_acc: 0.7365\n",
      "Epoch 62/200\n",
      "7079/7079 [==============================] - 197s 28ms/step - loss: 0.2736 - acc: 0.9453 - val_loss: 1.8401 - val_acc: 0.7194\n",
      "Epoch 63/200\n",
      "7079/7079 [==============================] - 196s 28ms/step - loss: 0.2795 - acc: 0.9438 - val_loss: 1.8124 - val_acc: 0.7243\n",
      "Epoch 64/200\n",
      "7079/7079 [==============================] - 197s 28ms/step - loss: 0.2889 - acc: 0.9434 - val_loss: 1.6796 - val_acc: 0.7243\n",
      "Epoch 65/200\n",
      "7079/7079 [==============================] - 198s 28ms/step - loss: 0.2831 - acc: 0.9452 - val_loss: 1.9619 - val_acc: 0.7096\n",
      "Epoch 66/200\n",
      "7079/7079 [==============================] - 195s 28ms/step - loss: 0.2850 - acc: 0.9483 - val_loss: 1.7400 - val_acc: 0.6998\n",
      "Epoch 67/200\n",
      "7079/7079 [==============================] - 208s 29ms/step - loss: 0.2892 - acc: 0.9411 - val_loss: 1.9082 - val_acc: 0.6912\n",
      "Epoch 68/200\n",
      "7079/7079 [==============================] - 264s 37ms/step - loss: 0.2758 - acc: 0.9467 - val_loss: 1.8484 - val_acc: 0.7341\n",
      "Epoch 69/200\n",
      "7079/7079 [==============================] - 268s 38ms/step - loss: 0.2723 - acc: 0.9499 - val_loss: 1.7395 - val_acc: 0.7206\n",
      "Epoch 70/200\n",
      "7079/7079 [==============================] - 255s 36ms/step - loss: 0.2713 - acc: 0.9483 - val_loss: 1.7970 - val_acc: 0.7316\n",
      "Epoch 71/200\n",
      "7079/7079 [==============================] - 255s 36ms/step - loss: 0.2854 - acc: 0.9459 - val_loss: 1.9175 - val_acc: 0.7365\n",
      "Epoch 72/200\n",
      "7079/7079 [==============================] - 254s 36ms/step - loss: 0.3059 - acc: 0.9369 - val_loss: 1.9501 - val_acc: 0.6863\n",
      "Epoch 73/200\n",
      "7079/7079 [==============================] - 198s 28ms/step - loss: 0.2742 - acc: 0.9514 - val_loss: 1.7992 - val_acc: 0.7500\n",
      "Epoch 74/200\n",
      "7079/7079 [==============================] - 182s 26ms/step - loss: 0.2652 - acc: 0.9511 - val_loss: 1.6822 - val_acc: 0.7451\n",
      "Epoch 75/200\n",
      "7079/7079 [==============================] - 183s 26ms/step - loss: 0.2772 - acc: 0.9463 - val_loss: 1.8824 - val_acc: 0.7439\n",
      "Epoch 76/200\n",
      "7079/7079 [==============================] - 181s 26ms/step - loss: 0.2754 - acc: 0.9458 - val_loss: 2.2176 - val_acc: 0.7108\n",
      "Epoch 77/200\n",
      "7079/7079 [==============================] - 189s 27ms/step - loss: 0.2694 - acc: 0.9511 - val_loss: 1.6533 - val_acc: 0.7426\n",
      "Epoch 78/200\n",
      "7079/7079 [==============================] - 251s 35ms/step - loss: 0.2800 - acc: 0.9490 - val_loss: 1.9107 - val_acc: 0.7243\n",
      "Epoch 79/200\n",
      "7079/7079 [==============================] - 233s 33ms/step - loss: 0.2651 - acc: 0.9493 - val_loss: 1.7205 - val_acc: 0.7157\n",
      "Epoch 80/200\n",
      "7079/7079 [==============================] - 253s 36ms/step - loss: 0.2805 - acc: 0.9462 - val_loss: 1.9529 - val_acc: 0.7132\n",
      "Epoch 81/200\n",
      "7079/7079 [==============================] - 237s 33ms/step - loss: 0.2769 - acc: 0.9515 - val_loss: 2.1549 - val_acc: 0.6801\n",
      "Epoch 82/200\n",
      "7079/7079 [==============================] - 182s 26ms/step - loss: 0.2932 - acc: 0.9448 - val_loss: 2.1575 - val_acc: 0.6936\n",
      "Epoch 83/200\n",
      "7079/7079 [==============================] - 181s 26ms/step - loss: 0.2746 - acc: 0.9501 - val_loss: 1.8687 - val_acc: 0.7218\n",
      "Epoch 84/200\n",
      "7079/7079 [==============================] - 177s 25ms/step - loss: 0.2712 - acc: 0.9480 - val_loss: 2.0174 - val_acc: 0.7047\n",
      "Epoch 85/200\n",
      "7079/7079 [==============================] - 228s 32ms/step - loss: 0.2737 - acc: 0.9497 - val_loss: 1.9687 - val_acc: 0.7304\n",
      "Epoch 86/200\n",
      "7079/7079 [==============================] - 235s 33ms/step - loss: 0.2818 - acc: 0.9477 - val_loss: 1.8745 - val_acc: 0.7463\n",
      "Epoch 87/200\n",
      "7079/7079 [==============================] - 178s 25ms/step - loss: 0.2699 - acc: 0.9493 - val_loss: 2.2075 - val_acc: 0.6973\n",
      "Epoch 88/200\n",
      "7079/7079 [==============================] - 172s 24ms/step - loss: 0.2745 - acc: 0.9524 - val_loss: 1.8433 - val_acc: 0.6998\n",
      "Epoch 89/200\n",
      "7079/7079 [==============================] - 174s 25ms/step - loss: 0.2715 - acc: 0.9500 - val_loss: 1.7168 - val_acc: 0.7218\n",
      "Epoch 90/200\n",
      "7079/7079 [==============================] - 181s 26ms/step - loss: 0.2555 - acc: 0.9580 - val_loss: 1.9115 - val_acc: 0.7071\n",
      "Epoch 91/200\n",
      " 300/7079 [>.............................] - ETA: 3:03 - loss: 0.2474 - acc: 0.9667"
     ]
    }
   ],
   "source": [
    "history = m.fit(x=X_train, y=y_train, batch_size=300,\n",
    "                    epochs=200, verbose=True,           \n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=(X_val, y_val), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['acc'], label='Accuracy')\n",
    "plt.plot(history.history['val_acc'], label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "prediction = m.predict(X_test, batch_size=130, verbose=1)\n",
    "y_predict=np.array([ np.argmax(p) for p in prediction])\n",
    "y_test_label = np.array([np.argmax(y) for y in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "acc = np.sum(y_test_label==y_predict) / float(len(y_test_label))\n",
    "print('Accuracy: {:.2f}'.format(acc))\n",
    "\n",
    "cm = confusion_matrix(y_test_label, y_predict )\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 14}, fmt='g', linewidths=.5, \n",
    "            yticklabels=label_list,xticklabels=label_list)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
